# 05-1_ã€Pythonã€‘Search the list using Google API â€”â€” CLEAN FULL VERSION
# ä»•æ§˜:
# - äº‹å‰ã‚¹ã‚­ãƒ£ãƒ³ã§ å„ã‚·ãƒ¼ãƒˆã®ã€Œæœªå‡¦ç† / ç·è¡Œæ•°ã€ã‚’è¡¨ç¤ºï¼ˆåˆè¨ˆã‚‚ï¼‰
# - å‡¦ç†å¯¾è±¡ã¯ "searched_URL" ãŒç©ºã®è¡Œã®ã¿ï¼ˆæœªå‡¦ç†åˆ¤å®šï¼‰
# - å‡¦ç†ä»¶æ•°ã¯ 'all' ã¾ãŸã¯ æ•°å€¤ã§æŒ‡å®š
# - æœªå‡¦ç†ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ æŠ½å‡ºã§å‡¦ç†ï¼ˆé‡è¤‡ãªã—ï¼‰ã€‚æŠ½å‡ºä¾‹ã‚’è¡¨ç¤º
# - æ¤œç´¢çµæœãŒã‚¼ãƒ­ã§ã‚‚å¿…ãš "--- row_start ---" ã‚’æ›¸ãè¾¼ã‚“ã§ã€Œå‡¦ç†æ¸ˆã¿ã€ç—•è·¡ã‚’æ®‹ã™
# - Aã¸æ›¸ãæˆ»ã—: Excelã¯è©²å½“ã‚·ãƒ¼ãƒˆã‚’ç½®æ›ä¿å­˜ / CSVã¯ä¸Šæ›¸ãä¿å­˜
# - Bã¯ã€Œä»Šå›å‡¦ç†ã—ãŸåˆ†ã®ã¿ã€ã®ãƒ‡ãƒ«ã‚¿ãƒ­ã‚°ã‚’ CWD/log_Searched/ ã«å‡ºåŠ›ï¼ˆtimestamp & processed_atåˆ—ä»˜ä¸ï¼‰
# - ãƒ‰ãƒ¡ã‚¤ãƒ³é‡è¤‡ã¯â€œä»Šå›å‡¦ç†ãƒãƒƒãƒå†…â€ã§é‡è¤‡ã—ãªã„ã‚ˆã†ã«åˆ¶å¾¡ï¼ˆã‚·ãƒ¼ãƒˆå˜ä½ï¼‰

import os
import time
import random
import glob
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse

import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from tqdm import tqdm

# ==== ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã¨CSE IDã‚’å–å¾— ====
API_KEY = os.environ.get("google_search_api_key")
CSE_ID = os.environ.get("google_search_engine_id")
if not API_KEY or not CSE_ID:
    raise ValueError("APIã‚­ãƒ¼ã¾ãŸã¯CSE IDãŒç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“")

# ==== ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡ï¼ˆ429å¯¾ç­–ï¼‰ ====
QPM_TARGET = 20
BASE_DELAY = 60.0 / QPM_TARGET
MAX_RETRIES = 6
BACKOFF_FACTOR = 2.0
JITTER_RANGE = (0.05, 0.25)

# 1ãƒ—ãƒ­ã‚»ã‚¹ã§ä½¿ã„å›ã™
_GOOGLE_SERVICE = None

def throttle_wait(delay=BASE_DELAY):
    time.sleep(delay + random.uniform(*JITTER_RANGE))

# ==== é‡è¤‡å›é¿ã®ãŸã‚ã®ä¿å­˜ãƒ‘ã‚¹ç”Ÿæˆï¼ˆæ¥é ­è¾ã§é€£ç•ªï¼‰ ====

def get_unique_path_prefix(path_str: str) -> str:
    path = os.path.abspath(path_str)
    if not os.path.exists(path):
        return path
    d = os.path.dirname(path)
    base = os.path.basename(path)
    i = 1
    while True:
        candidate = os.path.join(d, f"{i:03d}_{base}")
        if not os.path.exists(candidate):
            return candidate
        i += 1

# ==== Googleæ¤œç´¢ ====

def google_search(query, api_key, cse_id, num=10):
    global _GOOGLE_SERVICE
    if _GOOGLE_SERVICE is None:
        _GOOGLE_SERVICE = build("customsearch", "v1", developerKey=api_key, cache_discovery=False)
    delay = BASE_DELAY
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            res = _GOOGLE_SERVICE.cse().list(q=query, cx=cse_id, num=num).execute()
            throttle_wait(BASE_DELAY)
            return [item["link"] for item in res.get("items", [])]
        except HttpError as e:
            status = getattr(e.resp, "status", None)
            if status == 429 or (status and 500 <= status < 600):
                sleep_s = delay + random.uniform(*JITTER_RANGE)
                print(f"[{status}] retry {attempt}/{MAX_RETRIES} after {sleep_s:.2f}s")
                time.sleep(sleep_s)
                delay *= BACKOFF_FACTOR
                continue
            raise
        except Exception:
            throttle_wait(BASE_DELAY)
            if attempt == MAX_RETRIES:
                raise
    return []

# ==== ç‹¬è‡ªãƒ‰ãƒ¡ã‚¤ãƒ³æŠ½å‡º ====

def get_domain(url):
    try:
        netloc = urlparse(url).netloc
        return netloc.lower().lstrip("www.")
    except Exception:
        return url

# =========================
# â‘  åŒéšå±¤ã®ã€Œãƒ•ã‚©ãƒ«ãƒ€ã€ã‚’åˆ—æŒ™ã—ã¦é¸æŠï¼ˆallãªã—ãƒ»ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå¯ï¼‰
# =========================
SCRIPT_DIR = Path(__file__).resolve().parent
dirs_1depth = sorted([p for p in SCRIPT_DIR.iterdir() if p.is_dir()])

if not dirs_1depth:
    raise FileNotFoundError("åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã«ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

print("å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠã—ã¦ãã ã•ã„:")
for i, d in enumerate(dirs_1depth, start=1):
    print(f"{i}: {d.name}")
n_dirs = len(dirs_1depth)
raw_dir_pick = input(f"ç•ªå·ï¼ˆ1ã€œ{n_dirs}ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å¯ï¼‰: ").strip()
idxs = sorted({int(x.strip()) for x in raw_dir_pick.split(",") if x.strip().isdigit()})
if not idxs:
    raise ValueError(f"ãƒ•ã‚©ãƒ«ãƒ€ç•ªå·ã®å…¥åŠ›ãŒä¸æ­£ã§ã™ã€‚1ã€œ{n_dirs} ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
target_dirs = [dirs_1depth[i-1] for i in idxs if 1 <= i <= n_dirs]

# =========================
# â‘¡ é¸ã‚“ã ãƒ•ã‚©ãƒ«ãƒ€å†…ã§ Keyword-list_* ã‚’æ¢ã™ï¼ˆ1éšå±¤ä¸‹ã®ã¿ï¼‰
#    è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€ãã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã® .xlsx / .csv ã‚’å€™è£œã«
# =========================

def collect_candidate_files(base_dir: Path):
    cands = []
    cands += [Path(p) for p in glob.glob(str(base_dir / "Keyword-list_*.xlsx"))]
    cands += [Path(p) for p in glob.glob(str(base_dir / "Keyword-list_*.csv"))]
    if not cands:
        cands += [Path(p) for p in glob.glob(str(base_dir / "*.xlsx"))]
        cands += [Path(p) for p in glob.glob(str(base_dir / "*.csv"))]
    return sorted(cands)

# =========================
# â‘¢ å„ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«â†’ã‚·ãƒ¼ãƒˆ/CSVã‚’å‡¦ç†ï¼ˆäº‹å‰ã‚¹ã‚­ãƒ£ãƒ³â†’æœªå‡¦ç†ã ã‘å‡¦ç†ï¼‰
# =========================
for selected_dir in target_dirs:
    files = collect_candidate_files(selected_dir)
    if not files:
        print(f"[WARN] ãƒ•ã‚©ãƒ«ãƒ€ '{selected_dir.name}' ã«å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«(.xlsx/.csv)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        continue

    print("\n" + "="*72)
    print(f"â–¶ ãƒ•ã‚©ãƒ«ãƒ€: {selected_dir.name}")
    print("å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    for i, p in enumerate(files, start=1):
        print(f"{i}: {p.name}")
    n_files = len(files)
    raw_file_pick = input(f"ç•ªå·ï¼ˆ1ã€œ{n_files}ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å¯ï¼‰: ").strip()
    idxs = sorted({int(x.strip()) for x in raw_file_pick.split(",") if x.strip().isdigit()})
    if not idxs:
        raise ValueError(f"ãƒ•ã‚¡ã‚¤ãƒ«ç•ªå·ã®å…¥åŠ›ãŒä¸æ­£ã§ã™ã€‚1ã€œ{n_files} ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    target_files = [files[i-1] for i in idxs if 1 <= i <= n_files]

    for input_path in target_files:
        print("\n" + "-"*72)
        print(f"â–¶ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {input_path.name}")
        is_excel = input_path.suffix.lower() == ".xlsx"

        # ---- ã‚·ãƒ¼ãƒˆé¸æŠï¼ˆExcelã®ã¿ã¯ all å¯ï¼‰----
        if is_excel:
            excel = pd.ExcelFile(input_path)
            print("å‡¦ç†ã™ã‚‹ã‚·ãƒ¼ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„:")
            for idx, name in enumerate(excel.sheet_names, start=1):
                print(f"{idx}: {name}")
            n_sheets = len(excel.sheet_names)
            raw = input(f"ç•ªå·ï¼ˆ1ã€œ{n_sheets}ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š ã¾ãŸã¯ allï¼‰: ").strip().lower()
            if raw == "all":
                target_sheets = excel.sheet_names
            else:
                indices = []
                for token in raw.split(","):
                    token = token.strip()
                    if not token.isdigit():
                        raise ValueError(f"ä¸æ­£ãªç•ªå·å…¥åŠ›ã§ã™: {token}")
                    n = int(token)
                    if not (1 <= n <= n_sheets):
                        raise ValueError(f"ç•ªå·ãŒç¯„å›²å¤–ã§ã™: {n}ï¼ˆ1ã€œ{n_sheets}ï¼‰")
                    indices.append(n - 1)
                indices = sorted(set(indices))
                target_sheets = [excel.sheet_names[i] for i in indices]
        else:
            target_sheets = [None]  # CSV

        # ---- äº‹å‰ã‚¹ã‚­ãƒ£ãƒ³ï¼šå„ã‚·ãƒ¼ãƒˆ/CSVã®è¡Œæ•°ã¨æœªå‡¦ç†æ•°ã‚’å…ˆã«èª­ã¿è¾¼ã‚“ã§è¡¨ç¤º ----
        sheet_row_counts = {}
        sheet_remaining_counts = {}
        dfs_cache = {}
        total_selected_rows = 0
        total_remaining_rows = 0

        for sheet_name in target_sheets:
            if is_excel:
                df = pd.read_excel(input_path, sheet_name=sheet_name)
                label = sheet_name
            else:
                df = pd.read_csv(input_path)
                label = "CSV"

            if "searched_URL" not in df.columns:
                df["searched_URL"] = ""

            dfs_cache[label] = df
            total_rows = len(df)
            remaining_mask = df["searched_URL"].fillna("") == ""
            remaining = int(remaining_mask.sum())

            sheet_row_counts[label] = total_rows
            sheet_remaining_counts[label] = remaining
            total_selected_rows += total_rows
            total_remaining_rows += remaining

        print("------")
        print("é¸æŠã—ãŸã‚·ãƒ¼ãƒˆã”ã¨ã® æœªå‡¦ç† / ç·è¡Œæ•°:")
        for sheet in sheet_row_counts:
            print(f" - {sheet}: æœªå‡¦ç†={sheet_remaining_counts[sheet]} / ç·è¡Œæ•°={sheet_row_counts[sheet]}")
        print(f"â–¶ åˆè¨ˆ: æœªå‡¦ç†={total_remaining_rows} / ç·è¡Œæ•°={total_selected_rows}")

        # ---- å„ã‚·ãƒ¼ãƒˆ/CSVã®å‡¦ç†æœ¬ä½“ï¼ˆæœªå‡¦ç†ã®ã¿ã€ãƒ©ãƒ³ãƒ€ãƒ æŠ½å‡ºã€ä»¶æ•°æŒ‡å®šå¯ï¼‰ ----
        for sheet_name in target_sheets:
            if is_excel:
                label = sheet_name
                df = dfs_cache[label]
            else:
                label = "CSV"
                df = dfs_cache[label]

            total_rows = len(df)
            remaining_mask = df["searched_URL"].fillna("") == ""
            remaining_indices = list(df.index[remaining_mask])
            remaining = len(remaining_indices)

            print(f"\n[ {label} ] æœªå‡¦ç†: {remaining} / ç·è¡Œæ•°: {total_rows}")
            if remaining == 0:
                print("â†’ æœªå‡¦ç†è¡Œã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue

            # å‡¦ç†ä»¶æ•°ã®æŒ‡å®š
            ask = input(f"å‡¦ç†ã™ã‚‹è¡Œæ•°ã‚’å…¥åŠ›ï¼ˆ'all' ã¾ãŸã¯ æ•°å€¤ã€æœ€å¤§ {remaining}ï¼‰: ").strip().lower()
            if ask in ("", "all"):
                n_proc = remaining
            else:
                if not ask.isdigit():
                    raise ValueError(f"ä¸æ­£ãªå…¥åŠ›ã§ã™ï¼ˆall ã¾ãŸã¯ æ•°å€¤ï¼‰: {ask}")
                n_proc = max(0, min(int(ask), remaining))

            # ãƒ©ãƒ³ãƒ€ãƒ æŠ½å‡ºï¼ˆæœªå‡¦ç†ã‹ã‚‰é‡è¤‡ãªã—ã§ n_proc ä»¶ï¼‰
            if n_proc > 0:
                target_indices = random.sample(remaining_indices, k=n_proc)
                target_indices.sort()  # æ›¸ãæˆ»ã—æ™‚ã®è¦–èªæ€§ã®ãŸã‚æ˜‡é †
            else:
                target_indices = []
            example_rows = [(idx + 1) for idx in target_indices[:min(5, len(target_indices))]]
            print(f"â†’ ä»Šå›ã¯ ãƒ©ãƒ³ãƒ€ãƒ ã« {n_proc} è¡Œã‚’å‡¦ç†ã—ã¾ã™ã€‚ä¾‹: {example_rows}")

            # æ¤œç´¢å®Ÿè¡Œï¼ˆä»Šå›å‡¦ç†åˆ†ï¼‰
            all_domains = set()  # åŒä¸€ã‚·ãƒ¼ãƒˆå†…ã®ä»Šå›ã®å‡¦ç†ã§ãƒ‰ãƒ¡ã‚¤ãƒ³é‡è¤‡ã‚’é¿ã‘ã‚‹
            it = tqdm(range(n_proc), total=n_proc, desc=f"Googleæ¤œç´¢ä¸­ [{label}]")
            for k in it:
                i = target_indices[k]
                row = df.iloc[i]
                # å…ˆé ­3åˆ—ã‚’ã‚¯ã‚¨ãƒªã«ä½¿ã†ï¼ˆå­˜åœ¨ã—ãªã„åˆ—ã¯ç„¡è¦–ï¼‰
                cols = [row.iloc[j] if j < len(row) else None for j in range(3)]

                # ã‚¯ã‚¨ãƒªãŒç©ºãªã‚‰å‡¦ç†æ¸ˆã¿ãƒãƒ¼ã‚¯ã®ã¿
                if all(pd.isna(x) or str(x).strip() == "" for x in cols):
                    df.at[i, "searched_URL"] = "--- row_start ---"
                    continue

                query = " ".join([str(x) for x in cols if pd.notna(x) and str(x).strip()])
                try:
                    urls = google_search(query, API_KEY, CSE_ID, num=10)
                except Exception as e:
                    print(f"[WARN] æ¤œç´¢å¤±æ•—: {query} :: {e}")
                    urls = []
                urls_cleaned = [u.strip() for u in urls if u.strip()]

                if urls_cleaned:
                    content = "--- row_start ---\n" + "\n".join(urls_cleaned)
                else:
                    content = "--- row_start ---"  # çµæœã‚¼ãƒ­ã§ã‚‚å‡¦ç†æ¸ˆã¿ç—•è·¡
                # ãƒ‰ãƒ¡ã‚¤ãƒ³é‡è¤‡é™¤å»ï¼ˆä»Šå›å‡¦ç†åˆ†ã«å¯¾ã—ã¦ï¼‰
                if content.strip() != "--- row_start ---":
                    lines = content.split("\n")
                    header = lines[0]
                    uniq_urls = []
                    for url in lines[1:]:
                        domain = get_domain(url)
                        if domain not in all_domains:
                            uniq_urls.append(url)
                            all_domains.add(domain)
                    content = "\n".join([header] + uniq_urls) if uniq_urls else header

                df.at[i, "searched_URL"] = content

            # ==== (1) Aã¸æ›¸ãæˆ»ã—ï¼ˆä¸Šæ›¸ãï¼‰====
            if is_excel:
                from openpyxl import load_workbook  # ç¢ºå®Ÿã«openpyxlã‚’ä½¿ã†
                # æ—¢å­˜ãƒ–ãƒƒã‚¯ã®å½“è©²ã‚·ãƒ¼ãƒˆã‚’ç½®æ›ä¿å­˜ï¼ˆä»–ã‚·ãƒ¼ãƒˆã¯ä¿æŒï¼‰
                with pd.ExcelWriter(input_path, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    df.to_excel(writer, sheet_name=label, index=False)
                print(f"ğŸ’¾ Aã¸æ›¸ãæˆ»ã—å®Œäº† â†’ {input_path.name} / {label}")
            else:
                # CSV ã®å ´åˆã¯ A=CSV ã‚’ãã®ã¾ã¾ä¸Šæ›¸ã
                df.to_csv(input_path, index=False, encoding="utf-8-sig")
                print(f"ğŸ’¾ A(CSV) ã‚’ä¸Šæ›¸ãä¿å­˜ â†’ {input_path.name}")

            # ==== (2) B: ãƒ­ã‚°ã‚’ CWD/log_Searched/ ã«ä¿å­˜ ====
# ä»•æ§˜: ãƒ•ã‚¡ã‚¤ãƒ«Aï¼ˆå¯¾è±¡ã‚·ãƒ¼ãƒˆï¼‰ã¨åŒã˜è¡Œãƒ»åˆ—æ§‹é€ ã‚’â€œç©ºæ¬„ã§â€è¸è¥²ã—ã€
#       ä»Šå›å‡¦ç†ã—ãŸè¡Œã ã‘ã‚ªãƒªã‚¸ãƒŠãƒ«å†…å®¹ã‚’è»¢è¨˜ã™ã‚‹ï¼ˆï¼ä½ç½®ãŒåˆ†ã‹ã‚‹ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚°ï¼‰
run_ts = datetime.now().strftime("%Y%m%d-%H%M%S")

# Aã¨åŒã˜å½¢ï¼ˆå…¨ã‚»ãƒ«ç©ºæ–‡å­—ï¼‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”¨æ„
# â€»å…ƒã®dfã¯ä»Šå›æ™‚ç‚¹ã®æœ€æ–°ï¼ˆæ›¸ãæˆ»ã—åæ˜ æ¸ˆã¿ï¼‰
df_log = pd.DataFrame("", index=df.index, columns=df.columns)

# ä»Šå›å‡¦ç†ã—ãŸè¡Œã ã‘ã€å…ƒdfã®å…¨åˆ—ã‚’ãã®ã¾ã¾è»¢è¨˜
if target_indices:
    df_log.loc[target_indices, :] = df.loc[target_indices, :]

# ãƒ¡ã‚¿æƒ…å ±åˆ—ï¼ˆprocessed_atï¼‰ã‚’ä»˜ä¸ï¼ˆæœªå‡¦ç†è¡Œã¯ç©ºï¼‰
if "processed_at" not in df_log.columns:
    df_log["processed_at"] = ""
if target_indices:
    df_log.loc[target_indices, "processed_at"] = run_ts

# å‡ºåŠ›å…ˆ: ã‚«ãƒ¬ãƒ³ãƒˆç›´ä¸‹ log_Searched/
log_dir = input_path.parent / "log_Searched"
log_dir.mkdir(parents=True, exist_ok=True)

if is_excel:
    out_name = f"searched({label})_{input_path.stem}__log_{run_ts}.xlsx"
    output_path = log_dir / out_name
    with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
        df_log.to_excel(writer, sheet_name=label, index=False)
    print(f"ğŸ“ ãƒ­ã‚°å‡ºåŠ›ï¼ˆB: ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚°ï¼‰: {output_path}")
else:
    out_name = f"searched({label})_{input_path.stem}__log_{run_ts}.csv"
    output_path = log_dir / out_name
    df_log.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ ãƒ­ã‚°å‡ºåŠ›ï¼ˆB/CSV: ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚°ï¼‰: {output_path}")

print("\nã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
